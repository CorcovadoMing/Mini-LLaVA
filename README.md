<!-- <div style="display: flex; align-items: center; margin-bottom: 20px;"> -->
<div align="center">
  <h1 style="margin: 0;">Mini-LLaVA</h1>
  <img src="https://github.com/user-attachments/assets/45681a03-d10f-4e54-ba58-f858dde11dfd" width="300" alt="mice-tele" style="margin-right: 20px;">
    <img src="[https://github.com/user-attachments/assets/45681a03-d10f-4e54-ba58-f858dde11dfd](https://github.com/user-attachments/assets/7dacebcd-d3bd-4abf-aa52-199e0be04dec)" width="300" alt="mice-tele" style="margin-right: 20px;">

  <div>
    <p style="margin: 0;">Minimal Implementation of LLaVA (Large Language and Vision Assistant) with interleaved image, text, and video processing ability.</p>
  </div>
</div>
## Overview


Mini-LlaVA is a lightweight implementation of the LLaVA model, designed to unlock multimodal capabilities of Large Language Models (LLMs) using a single GPU. This project extends the original LLaVA concept by enabling interleaved processing of multiple images, videos, and text inputs respecting their order of appearance.

## Features

- Minimal implementation of LLaVA
- Interleaved processing of multiple modalities of any number, obeying order of their inputs:
  - Images
  - Videos
  - Text
- Jupyter notebook for easy experimentation and learning

## Environment Set-up
```shell
run set.sh
```
