# Mini-LlaVA

Minimal Implementation of LLaVA (Large Language and Vision Assistant) with interleaved image, text, and video processing ability.

## Overview

Mini-LlaVA is a lightweight implementation of the LLaVA model, designed to unlock multimodal capabilities of Large Language Models (LLMs) using a single GPU. This project extends the original LLaVA concept by enabling interleaved processing of multiple images, videos, and text inputs.

## Features

- Minimal implementation of LLaVA
- Interleaved processing of multiple modalities of any number, obeying order of their inputs:
  - Images
  - Videos
  - Text
- Jupyter notebook for easy experimentation and learning
